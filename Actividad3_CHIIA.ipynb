{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gerpervaz/naturalprocessinglanguage/blob/main/Actividad3_CHIIA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_JRPIaFSfqN"
      },
      "source": [
        "1. ADQUISICIÓN DE LOS DATOS\n",
        "\n",
        "Creamos el dataframe \"df\" a partir del archivo csv de interés. Hay tres opciones de subir el archivo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZFK7ISk-nl6"
      },
      "source": [
        "OPCIÓN A) ADQUISICIÓN DE LOS DATOS DESDE GOOGLE DRIVE (más adecuado para archivos grandes, ya que es más rápido  y no usa su tarifa de datos). Léase antes de ejecutar.\n",
        " \n",
        "1. Requiere colocar el csv en su google drive.\n",
        "2. Cuando se ejecute se le pedirá aceptar permisos para montar su drive en la maquina virtual-instancia a la que está conectado/a.\n",
        "3. Sus archivos siguen siendo personales, pero desgraciadamente drive.mount no permite montar subcarpetas del drive por lo que se monta el drive completo, por lo que no es recomendable esta opción si dispone de material especialmente sensible en su drive.\n",
        "4. Al terminar de ejecutar este bloque el google drive se desvinculará de la instancia guardando cualquier cambio hecho en Drive. Este código NO realiza cambios en su google drive, tan solo captura el archivo que contiene el dataset de interés. Sin embargo, cualquier cambio adicional que se realice intencionadamente o no sobre el resto de archivos del drive quedará guardado tras la ejecución de la última celda de este apartado. Si esto sucediera, y quisiera descartar los cambios, debería hacerlo directamente en Google Drive yendo a la ubicación del archivo, haciendo clic con el botón derecho sobre él y seleccionando \"Administrar versiones\" para revertir a una versión anterior.\n",
        "5. Debido a lo anterior, es recomendable tras ejecutar este bloque de código, consultar el historial de actividad de su Google Drive siguiendo estos pasos: Inicie sesión en la web de Google Drive, En la esquina superior derecha, haga clic en el ícono de información (una \"i\" dentro de un círculo). En el panel que se abre a la derecha, seleccione la pestaña \"Actividad\". Podrá ver una lista de eventos recientes en su Google Drive, como la creación de archivos, modificaciones, eliminaciones y movimientos de archivos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4-8O5xxGs0Z",
        "outputId": "b0c15fa6-4297-4ccf-eb8d-c67ec770269d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yd06gYuMT0Eh",
        "outputId": "d9d730e4-fe8c-4889-f450-55041ba8b569"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PuGvLVpNkPQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/marketing_sample_for_booking_com-travel__20190501_20190630__30k_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYN-skr2buNy",
        "outputId": "9e167d0c-4ce6-4714-9e2f-03b004d920fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Google Drive desmontado exitosamente.\n"
          ]
        }
      ],
      "source": [
        "# Desmontar el Google Drive (Desmontar ahora o desmontar más tarde si se desean guardar resultados en el drive)\n",
        "drive.flush_and_unmount()\n",
        "print(\"Google Drive desmontado exitosamente.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Xih6gRqNXGt"
      },
      "source": [
        "OPCIÓN B) DESDE SU ENTORNO LOCAL DE PC A COLAB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HYvvnAnw2MIb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Crear la carpeta donde se guardarán los archivos importados\n",
        "folder_name = \"imported_files\"\n",
        "if not os.path.exists(folder_name):\n",
        "    os.makedirs(folder_name)\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Obtener el nombre del archivo subido\n",
        "file_name = list(uploaded.keys())[0]\n",
        "\n",
        "# Obtener la ruta completa del archivo en la carpeta especificada\n",
        "file_path = os.path.join(folder_name, file_name)\n",
        "\n",
        "# Mover el archivo a la carpeta especificada\n",
        "shutil.move(file_name, file_path)\n",
        "\n",
        "# Imprimir la ruta completa del archivo\n",
        "print(f\"El archivo {file_name} se ha guardado en la ruta: {file_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttxAdB_36r-B"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/imported_files/marketing_sample_for_booking_com-travel__20190501_20190630__30k_data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJv9wjgC5-rr"
      },
      "source": [
        "OPCIÓN C) TRABAJAR EN ENTORNO LOCAL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrtstNUk67Zj"
      },
      "outputs": [],
      "source": [
        "#Sustituir ruta_al_archivo por la ruta donde se encuentre marketing_sample_for_booking_com-travel__20190501_20190630__30k_data.csv\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('ruta_al_archivo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gj9EcwHsRo9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "# Leemos los datos desde el archivo csv\n",
        "df = pd.read_csv('/content/marketing_sample_for_booking_com-travel__20190501_20190630__30k_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etJO_tqfZE4q",
        "outputId": "e3069bdb-ea75-49ee-ace2-0e2b7dc812e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El número de registros del DataFrame es: 30000\n"
          ]
        }
      ],
      "source": [
        "num_registros = df.shape[0]\n",
        "print(\"El número de registros del DataFrame es:\", num_registros)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. ANÁLISIS EXPLORATORIO DE LOS DATOS**\n",
        "\n",
        "Este bloque de código muestra distintas opciones de resumir nuestros datos con el fin de facilitar la comprensión de la tipología y formato de los mismos, así como identificar posibles errores susceptibles de ser depurados en el siguiente apartado."
      ],
      "metadata": {
        "id": "AIcFQ-9YQRT_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizar los primeros 10 resultados de todas las columnas del dataframe"
      ],
      "metadata": {
        "id": "NZ92goK7Qthg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQJRkZ3fZX-X"
      },
      "outputs": [],
      "source": [
        "#\"import pandas as pd\" si no se ha ejecutado antes.\n",
        "\n",
        "# Ajustar la opción 'display.max_columns' para mostrar todas las columnas\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# Ajustar la opción 'display.max_colwidth' para aumentar la anchura de la columna\n",
        "pd.set_option('display.max_colwidth', 80)  # puedes ajustar este número según tus necesidades\n",
        "\n",
        "print(df.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizar el tipo de dato almacenado en cada columna/campo/variable."
      ],
      "metadata": {
        "id": "nfeomI6QRVVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#imprime el tipo de dato almacenado en cada campo según formato pandas\n",
        "df.dtypes"
      ],
      "metadata": {
        "id": "x2qGW_3eLBk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Otra forma de visualizar el tipo de dato que contiene cada. Y además, la longitud máxima de los campos tipo cadena, y si hay valores desconocidos."
      ],
      "metadata": {
        "id": "gs-eWPznRd5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprime el tipo de dato que contiene el campo, la longitud máxima de los campos tipo cadena, y si hay valores desconocidos.\n",
        "\n",
        "info_columnas = []\n",
        "for columna in df.columns:\n",
        "    tipo = str(df[columna].dtype)\n",
        "    if tipo == 'object':\n",
        "        longitud = df[columna].astype(str).apply(len).max()\n",
        "        faltantes = df[columna].isnull().sum()\n",
        "        vacios = (df[columna] == '').sum()\n",
        "        info_columnas.append((columna, tipo, longitud, faltantes, vacios))\n",
        "    elif tipo.startswith('datetime'):  # Si la columna es de tipo fecha\n",
        "        longitud = 'N/A'  # No tiene sentido calcular la longitud para tipos fecha\n",
        "        faltantes = df[columna].isnull().sum()\n",
        "        info_columnas.append((columna, tipo, longitud, faltantes, None))\n",
        "    else:\n",
        "        if df[columna].isnull().all():  # Si todos los valores son NaN\n",
        "            tipo = 'numérico o string'\n",
        "        longitud = 'N/A'  # No tiene sentido calcular la longitud para tipos numéricos\n",
        "        faltantes = df[columna].isnull().sum()\n",
        "        info_columnas.append((columna, tipo, longitud, faltantes, None))\n",
        "\n",
        "for columna, tipo, longitud, faltantes, vacios in info_columnas:\n",
        "    if tipo == 'object':\n",
        "        print(f\"La columna '{columna}' es de tipo string, longitud máxima: {longitud}\")\n",
        "        print(f\"Valores faltantes: {faltantes}\")\n",
        "        print(f\"Valores vacíos: {vacios}\")\n",
        "    elif tipo == 'numérico o string':\n",
        "        print(f\"La columna '{columna}' podría ser de tipo numérico o string\")\n",
        "        print(f\"Valores faltantes: {faltantes}\")\n",
        "    elif tipo.startswith('datetime'):\n",
        "        print(f\"La columna '{columna}' es de tipo fecha\")\n",
        "        print(f\"Valores faltantes: {faltantes}\")\n",
        "    else:\n",
        "        print(f\"La columna '{columna}' es de tipo numérico\")\n",
        "        print(f\"Valores faltantes: {faltantes}\")\n"
      ],
      "metadata": {
        "id": "eUb7cwuL-5rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estadísticia básica de cada campo mediante proceidmiento \"describe()\""
      ],
      "metadata": {
        "id": "Vv4uqiUnTb_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Describir el archivo incluyendo estadísticas para todas las columnas (también las no numéricas).\n",
        "data_described=df.describe(include='all')\n",
        "\n",
        "# Ajustar la opción 'display.max_columns' para mostrar todas las columnas\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# Exploramos los datos imprimiendo el DataFrame generado por describe\n",
        "print(data_described)"
      ],
      "metadata": {
        "id": "sw7L2smcx6IG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Herramienta de visualización para Análisis Exploratorio de los Datos.** \n",
        "\n",
        "Nota: En caso de fallo en la generación del informe, eliminar versiones de informes anteriores que pueda haber en local. Si con esto no funciona, reiniciar el entorno de ejecución."
      ],
      "metadata": {
        "id": "_EyJOz1cXlVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Instalar ydata-profiling\n",
        "!pip install -U ydata-profiling[notebook]==4.1.0 #Esta versión se ha comprobado que funciona en Google Colab. La última versión daba ValueError por conflictos de \"TrueType fonts\"\n",
        "!pip show ydata-profiling"
      ],
      "metadata": {
        "id": "RrAhIUkJW8Cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generar el informe\n",
        "from ydata_profiling import ProfileReport\n",
        "profile = ProfileReport(df, title=\"Pandas Profiling Report\")"
      ],
      "metadata": {
        "id": "keFC9ieT670f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Genera un archivo html que se guarda en local en el directorio activo\n",
        "profile.to_file(\"dataset_EDA_report.html\")\n",
        "\n",
        "# Si se quiere visualizar en la celda del notebook\n",
        "profile.to_notebook_iframe()\n",
        "\n",
        "#Si estás en entorno de jupyter notebook puedes ejecutar la siguiente instrucción.\n",
        "#profile.to_widgets()"
      ],
      "metadata": {
        "id": "xmwhMqKl8m03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Otra herramienta para el Análisis Exploratorio de los Datos."
      ],
      "metadata": {
        "id": "Fdl6SMmT_wis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Primero, instala la biblioteca con pip\n",
        "!pip install sweetviz\n",
        "\n",
        "# Importa la biblioteca\n",
        "import sweetviz as sv\n",
        "\n",
        "# Analiza los datos\n",
        "report = sv.analyze(df)\n",
        "\n",
        "# Crea un informe en html\n",
        "report.show_html('Report.html')\n"
      ],
      "metadata": {
        "id": "NXttCVHY-rd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Código para explorar distintas estructuras de subcadenas en campos tipo cadena\n"
      ],
      "metadata": {
        "id": "K0RtjeCWUwR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Buscar coincidencias de subcadenas insensibles a mayúscula dentro de un campo de texto\n",
        "pd.set_option('display.max_colwidth', None)  # None significa sin límite\n",
        "\n",
        "na_values_pattern = \"^subcadena_que_buscamos$\" # Usa los símbolos ^ si quieres que empiece o $ si quieres que termine\n",
        "mask = df['Review Text Negative'].str.lower().str.contains(na_values_pattern, na=False)\n",
        "print(df['Review Text Negative'][mask])"
      ],
      "metadata": {
        "id": "90NptQiMiARv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO8n5DXxK3f7"
      },
      "source": [
        "Con display.max_rows vemos las categorías contenidas en las variables string, y si existe alguna categoría errónea. También sirve para ver opciones de agrupación o codificación numérica"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7UnTJRXBzmq"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_rows', None)\n",
        "df['Review Author Location'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N05J9DioEdLP"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_rows', None)\n",
        "df['Review Author Age'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. LIMPIEZA Y DEPURACIÓN DE ERRORES**"
      ],
      "metadata": {
        "id": "mCS-KmxBoujh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eliminar aquellas variables identificadas como irrelevantes"
      ],
      "metadata": {
        "id": "qtrWhEl4o044"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_drop = [\"Source\", \"Review Author Type\", \"Review Stay Date\", \"Review Id From Source\", \"Review Author Gender\", \n",
        "                   \"Review Locale\", \"Review Text Neutral\", \"Review Text Hotel Location\", \n",
        "                   \"Entry Added At\", \"Entry Processed At\", \"Review Misc\"]\n",
        "\n",
        "df = df.drop(columns=columns_to_drop)"
      ],
      "metadata": {
        "id": "nzFc75OfoxRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convertir en cadenas vacías los registros intranscendentes"
      ],
      "metadata": {
        "id": "1sHOMaAhZqBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Review Title'] = df['Review Title'].replace('NO REVIEWS IN DATE RANGE', '')"
      ],
      "metadata": {
        "id": "D0VAb-fvZoTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Limpieza y conversión a formato fecha de las variables tipo fecha"
      ],
      "metadata": {
        "id": "xiUDHTj56Pkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['Review Publication Date'] = pd.to_datetime(df['Review Publication Date'], format='%Y-%m-%d', errors='coerce')\n"
      ],
      "metadata": {
        "id": "dGt9Efx36L5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4e3fIl5XGKg"
      },
      "source": [
        "Depuramos el texto de las variables review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vOQAqssafrI",
        "outputId": "db6ed1d9-8db7-44b4-cf50-0b40a24108a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30000, 16)\n",
            "(6290, 16)\n"
          ]
        }
      ],
      "source": [
        "# No ejecutar\n",
        "# print(df.shape)  # Imprime el número de filas antes de eliminar los valores nulos\n",
        "\n",
        "# Elimina las filas con valores null en 'Review Text Positive' y 'Review Text Negative'\n",
        "# df = df.dropna(subset=['Review Text Positive', 'Review Text Negative'])\n",
        "\n",
        "# print(df.shape)  # Imprime el número de filas después de eliminar los valores nulos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49Sa5w_SWqCz"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "\n",
        "# Definimos los valores que consideramos como N/A o sin significado para la review negativa\n",
        "na_values = r'^(N/A|Na|na|NA|無|N\\\\A|N/a|Ñ/A|da|Nothing|Nothing.|nothing|None|Nada|nada|Rien|rien|Nulla|nulla|Niente|niente|ничего|Nichts|nichts|Niets)$'\n",
        "\n",
        "# Hacemos una copia solo de las columnas que nos interesan\n",
        "df_before = df[['Review Text Negative', 'Review Text Positive']].copy()\n",
        "\n",
        "# Reemplazamos los valores sin significado con una cadena vacía\n",
        "df['Review Text Negative'] = df['Review Text Negative'].str.strip().replace(na_values, '', regex=True)\n",
        "df['Review Text Positive'] = df['Review Text Positive'].str.strip().replace(na_values, '', regex=True)\n",
        "\n",
        "# Rellenamos los valores NaN con una cadena vacía\n",
        "df[\"Review Text Positive\"] = df[\"Review Text Positive\"].fillna(\"\")\n",
        "df[\"Review Text Negative\"] = df[\"Review Text Negative\"].fillna(\"\")\n",
        "\n",
        "# Calculamos cuántos registros se han modificado\n",
        "positive_changes = np.sum(df_before[\"Review Text Positive\"] != df[\"Review Text Positive\"])\n",
        "negative_changes = np.sum(df_before[\"Review Text Negative\"] != df[\"Review Text Negative\"])\n",
        "\n",
        "print(f'Se han modificado {positive_changes} registros en \"Review Text Positive\"')\n",
        "print(f'Se han modificado {negative_changes} registros en \"Review Text Negative\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplo de identificación de outliers en una variable categórica\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9LwI8m-Byzf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculamos las frecuencias de los nombres\n",
        "name_frequencies = df['Review Author Name'].value_counts()\n",
        "\n",
        "# Definimos el umbral como el percentil 99.9 de las frecuencias\n",
        "threshold = name_frequencies.quantile(0.999)\n",
        "\n",
        "# Identificamos los outliers\n",
        "outliers = name_frequencies[name_frequencies > threshold]\n",
        "\n",
        "print('Outliers identificados:\\n', outliers)\n"
      ],
      "metadata": {
        "id": "sO6kFucCy2WQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejemplo de identificación de outliers en una variable numérica"
      ],
      "metadata": {
        "id": "02XgOXU00goL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculamos el IQR de la variable 'Review Rating'\n",
        "Q1 = df['Review Rating'].quantile(0.25)\n",
        "Q3 = df['Review Rating'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Definimos los umbrales para considerar un valor como un outlier\n",
        "lower_threshold = Q1 - 1.5 * IQR\n",
        "upper_threshold = Q3 + 1.5 * IQR\n",
        "\n",
        "# Identificamos los outliers inferiores\n",
        "lower_outliers = df[df['Review Rating'] < lower_threshold]\n",
        "lower_outliers_values = lower_outliers['Review Rating']\n",
        "lower_outliers_ids = lower_outliers['Uniq Id']\n",
        "\n",
        "# Identificamos los outliers superiores\n",
        "upper_outliers = df[df['Review Rating'] > upper_threshold]\n",
        "upper_outliers_values = upper_outliers['Review Rating']\n",
        "upper_outliers_ids = upper_outliers['Uniq Id']\n",
        "\n",
        "# Imprimimos los outliers inferiores\n",
        "if lower_outliers.empty:\n",
        "    print('No se encuentran outliers inferiores.')\n",
        "else:\n",
        "    print('Outliers inferiores identificados:')\n",
        "    for value, idx in zip(lower_outliers_values, lower_outliers_ids):\n",
        "        print(f\"Valor: {value}, Identificador: {idx}\")\n",
        "\n",
        "# Imprimimos los outliers superiores\n",
        "if upper_outliers.empty:\n",
        "    print('No se encuentran outliers superiores.')\n",
        "else:\n",
        "    print('\\nOutliers superiores identificados:')\n",
        "    for value, idx in zip(upper_outliers_values, upper_outliers_ids):\n",
        "        print(f\"Valor: {value}, Identificador: {idx}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "1-TqmKgc2VdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxvSnUYsGVxb"
      },
      "source": [
        "# **4. TRANSFORMACIÓN DE LOS DATOS (Recodificación y creación de nuevas variables)**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.1. Recodificación de las categorías de edad a numérico, para potenciales análisis de regresión posterior"
      ],
      "metadata": {
        "id": "RMhvg2AOBPwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "age_map = {\n",
        "    '18 – 24': 0,\n",
        "    '25 – 34': 1,\n",
        "    '35 – 44': 2,\n",
        "    '45 – 54': 3,\n",
        "    '55 – 64': 4,\n",
        "    '65+': 5\n",
        "}\n",
        "df['Review Author Age'] = df['Review Author Age'].map(age_map)\n"
      ],
      "metadata": {
        "id": "lGgYc-dvBDZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2wh5A4BOkx6"
      },
      "source": [
        "4.2. Extracción de país del hotel a partir de su url, y añadir columna nueva con el dato."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HM0-yG-HLEDo"
      },
      "outputs": [],
      "source": [
        "# Extrae los dos dígitos siguientes a '/reviews/' en la columna 'Hotel Url From Source'\n",
        "df['country'] = df['Review Url'].str.extract('/reviews/([a-zA-Z]{2})', expand=False)\n",
        "\n",
        "# Genera una máscara booleana con valores faltantes\n",
        "mask = df.isnull().all()\n",
        "\n",
        "# Selecciona las columnas con valores faltantes\n",
        "col_empty = df.columns[mask]\n",
        "\n",
        "# Elimina las columnas vacías del DataFrame\n",
        "df = df.drop(col_empty, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.3. Extraer el tipo de alojamiento y recodificar en one-hot econding"
      ],
      "metadata": {
        "id": "Gyb1w9BUjxkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardamos el número original de columnas\n",
        "original_num_columns = df.shape[1]\n",
        "\n",
        "# Primero, dividimos la columna \"Review Stay Details\" en múltiples columnas \n",
        "# utilizando el símbolo \"|\" como separador.\n",
        "df_temp = df['Review Stay Details'].str.split('|', expand=True)\n",
        "\n",
        "# Aplicamos la función get_dummies a cada una de las columnas recién creadas\n",
        "# luego las sumamos. Esto resultará en un DataFrame donde cada frase única \n",
        "# es una columna, y los valores son 0 o 1 dependiendo de si la frase aparece \n",
        "# en la fila original.\n",
        "df_final = pd.get_dummies(df_temp.apply(pd.Series).stack()).groupby(level=0).sum()\n",
        "\n",
        "# Ahora, puedes juntar este DataFrame (df_final) con tu DataFrame original (df) si lo necesitas.\n",
        "df = pd.concat([df, df_final], axis=1)\n",
        "\n",
        "# Calculamos el número de nuevas columnas generadas\n",
        "new_columns_generated = df.shape[1] - original_num_columns\n",
        "\n",
        "# Imprimimos el número de nuevas columnas generadas\n",
        "print(\"Número de nuevas columnas generadas: \", new_columns_generated)\n"
      ],
      "metadata": {
        "id": "FESw-nI8jzUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizar las nuevas variables creadas"
      ],
      "metadata": {
        "id": "qIXNW3XNedwZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gráfico (opción 1) de barras ordenadas en función de la frecuencia absoluta de valores \"1\" en las columnas. Usar N para ajustar el número de columnas que se quieren mostrar."
      ],
      "metadata": {
        "id": "8d__aM_Ufttv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solo las primeras N columnas\n",
        "N = 200\n",
        "\n",
        "# Crea un nuevo índice para las primeras N columnas que es solo el número de la columna\n",
        "sorted_sums.index = range(1, len(sorted_sums) + 1)\n",
        "\n",
        "# Crea un diagrama de barras para las primeras N columnas\n",
        "plt.figure(figsize=(10, 6))\n",
        "sorted_sums[:N].plot(kind='bar')\n",
        "plt.title('Frecuencia de \"1\"s en las primeras {} columnas'.format(N))\n",
        "plt.xlabel('Número de columna')\n",
        "plt.ylabel('Frecuencia')\n",
        "\n",
        "# Configura las etiquetas del eje x para mostrar cada 100 columnas\n",
        "plt.xticks(range(0, N, 100), range(1, N + 1, 100))\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "WZdF4YQG37iF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gráfico (opción 2) de barras ordenadas en función de la frecuencia absoluta de valores \"1\" en las columnas. En el eje secundario se muestra también la proporción de valores \"1\" en cada variable.  Usar N para ajustar el número de columnas que se quieren mostrar."
      ],
      "metadata": {
        "id": "F5z86rR6gB2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Solo las primeras N columnas\n",
        "N = 200\n",
        "\n",
        "# Cálculo de la media de cada columna\n",
        "column_means = df_final.mean()\n",
        "\n",
        "# Suma cada columna para obtener la cantidad de \"1\"s en cada una\n",
        "column_sums = df_final.sum()\n",
        "\n",
        "# Ordena las sumas\n",
        "sorted_sums = column_sums.sort_values(ascending=False)\n",
        "\n",
        "# Ordena las medias de la misma manera que las sumas\n",
        "sorted_means = column_means[sorted_sums.index]\n",
        "\n",
        "# Crea un nuevo índice para las primeras N columnas que es solo el número de la columna\n",
        "sorted_sums.index = range(1, len(sorted_sums) + 1)\n",
        "sorted_means.index = range(1, len(sorted_means) + 1)\n",
        "\n",
        "# Crea la figura y un primer eje para las frecuencias\n",
        "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Dibuja las frecuencias en el primer eje\n",
        "ax1.bar(sorted_sums.index[:N], sorted_sums[:N], color='b')\n",
        "ax1.set_title('Frecuencia y Proporción de \"1\"s en las primeras {} columnas'.format(N))\n",
        "ax1.set_xlabel('Número de columna')\n",
        "ax1.set_ylabel('Frecuencia', color='b')\n",
        "ax1.tick_params('y', colors='b')\n",
        "\n",
        "# Configura las etiquetas del eje x para mostrar cada 50 columnas\n",
        "ax1.set_xticks(range(0, N + 1, 50))\n",
        "\n",
        "# Crea un segundo eje para las medias\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "# Dibuja las medias en el segundo eje\n",
        "ax2.plot(sorted_means.index[:N], sorted_means[:N], color='r')\n",
        "ax2.set_ylabel('Proporción', color='r')\n",
        "\n",
        "# Establecer ticks mayores cada 0.2 en el eje Y secundario\n",
        "ax2.set_yticks(np.arange(0, sorted_means.max() + 0.1, 0.2))\n",
        "\n",
        "# Establecer ticks menores cada 0.1 en el eje Y secundario\n",
        "ax2.set_yticks(np.arange(0, sorted_means.max() + 0.1, 0.1), minor=True)\n",
        "\n",
        "# Cambiar el color de los ticks a rojo\n",
        "ax2.tick_params('y', colors='r')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jw5sFBzI9Azv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reagrupación de las variables dummy generadas en función de un umbral (5%) de proporción de valores \"1\". Las variables por debajo del umbral se agrupan en una única variable que representa la categoría \"Others\""
      ],
      "metadata": {
        "id": "Z6Te33oBhCgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cálculo de la media de cada columna\n",
        "column_means = df_final.mean()\n",
        "\n",
        "# Suma cada columna para obtener la cantidad de \"1\"s en cada una\n",
        "column_sums = df_final.sum()\n",
        "\n",
        "# Define el umbral en términos de porcentaje\n",
        "threshold = 0.05 * len(df_final)\n",
        "\n",
        "# Identifica las columnas que contienen menos del 5% de \"1\"\n",
        "low_freq_cols = column_sums[column_sums < threshold].index\n",
        "\n",
        "# Crea una nueva columna que es la suma de las columnas de baja frecuencia\n",
        "df_final['Others'] = df_final[low_freq_cols].max(axis=1)\n",
        "\n",
        "# Elimina las columnas de baja frecuencia\n",
        "df_final = df_final.drop(low_freq_cols, axis=1)\n"
      ],
      "metadata": {
        "id": "zMdz2Q0F8pnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se recomienda volver a graficar tras la conversión."
      ],
      "metadata": {
        "id": "5MpNcGoc9Ccz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6g4rlIhFXIr-"
      },
      "source": [
        "4.4. Creamos la variable que detecta el idioma de las variables con el texto de las reseñas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fStz10LVaed8"
      },
      "outputs": [],
      "source": [
        "!pip install langdetect\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwK6i9CJfH6d"
      },
      "outputs": [],
      "source": [
        "#Detección del idioma con calculo paralelizado para acelerar el proceso\n",
        "from langdetect import detect\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import numpy as np\n",
        "\n",
        "def detect_language(text):\n",
        "    try:\n",
        "        return detect(text)\n",
        "    except:\n",
        "        return 'unknown'\n",
        "\n",
        "def parallelize_dataframe(df, func, n_cores=4):\n",
        "    df_split = np.array_split(df, n_cores)\n",
        "    pool = ProcessPoolExecutor(n_cores)\n",
        "    df = pd.concat(pool.map(func, df_split))\n",
        "    pool.shutdown()\n",
        "    return df\n",
        "\n",
        "def apply_detect_language(df):\n",
        "    df[\"Language_Review Text Positive\"] = df[\"Review Text Positive\"].apply(detect_language)\n",
        "    df[\"Language_Review Text Negative\"] = df[\"Review Text Negative\"].apply(detect_language)\n",
        "    return df\n",
        "\n",
        "df = parallelize_dataframe(df, apply_detect_language)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZajIy98h71m"
      },
      "outputs": [],
      "source": [
        "#imprime los campos de idioma de las 20 primeras filas\n",
        "print(df[['Review Text Positive', 'Language_Review Text Positive', 'Review Text Negative', 'Language_Review Text Negative',]].head(20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVkun_GyZyee"
      },
      "source": [
        "Añadir comprobación de excluir aquel idioma que no coincida entre comentarios positivos y negativos"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Opcionalmente guardar un archivo csv intermedio con el procesamiento hasta el momento."
      ],
      "metadata": {
        "id": "86WLKthRG6WT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zF7J9CiyhL6m"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"/content/comments_with_lang.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Opcionalmente utilizar el csv intermedio generado hasta el momento, para seguir desde este punto."
      ],
      "metadata": {
        "id": "c09a7TtQHBWA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BK3CKiGryKOt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/comments_with_lang.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40O6SnXIPClU"
      },
      "source": [
        "4.5. Opcionalmente podemos transformar los comentarios y quedarnos sólo con la primera frase del texto de las reseñas. Si se quisieran llevar a cabo los análisis solo sobre esta primera frase del texto, haría falta modificar el código posterior para utilizar los campos \"1st Sentence\" como input, en lugar de los originales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLE08C18q9Hq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Función para extraer la primera frase\n",
        "def extraer_primera_frase(texto):\n",
        "    # Utilizar expresiones regulares para encontrar la primera frase\n",
        "    primera_frase = re.match(r'^.*?[.!?]', texto)\n",
        "    if primera_frase:\n",
        "        return primera_frase.group(0)\n",
        "    else:\n",
        "        return texto  # Si no se encuentra ninguna frase, devuelve el texto completo\n",
        "\n",
        "# Aplicar la función a los campos 'Review Text Positive' y 'Review Text Negative'\n",
        "df['Review Text Positive 1st Sentence'] = df['Review Text Positive'].apply(extraer_primera_frase)\n",
        "df['Review Text Negative 1st Sentence'] = df['Review Text Negative'].apply(extraer_primera_frase)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqLFnNbATyZ6"
      },
      "source": [
        "# 4.5. CREACIÓN DE LAS NUEVAS VARIABLES PARA LA CLASIFICACIÓN DE LA TEMÁTICA DE LAS RESEÑAS MEDIANTE ENFOQUE ZERO-SHOT A PARTIR DE UN PIPELINE DE HUGGING FACE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalamos la librería transformers de Hugging Face y las dependencias necesarias"
      ],
      "metadata": {
        "id": "NKo0e-daEbHU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HlfSw-xjXH1K"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yAd7TT8WuJx"
      },
      "outputs": [],
      "source": [
        "# importamos pipeline\n",
        "from transformers import pipeline\n",
        "# \"import pandas as pd\" si no se ha importado antes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comprobamos "
      ],
      "metadata": {
        "id": "eeOhZHx6EZOM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BASCZt1Po1v"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print(\"GPU Available: \", len(tf.config.list_physical_devices('GPU')) > 0)\n",
        "print(tf.test.is_gpu_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAm_2OThQx1Y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlE58_pbY6wS"
      },
      "outputs": [],
      "source": [
        "!pip install sentencepiece\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4f9qezHWvfG"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "# Crear el pipeline de clasificación # device=0 para usar la GPU\n",
        "classifier = pipeline(\"zero-shot-classification\", model='joeddav/xlm-roberta-large-xnli', use_auth_token='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', device=0, multi_label=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJ3sLqw1fTf6"
      },
      "outputs": [],
      "source": [
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBg8ZyNp91E4"
      },
      "source": [
        "PARA LOS COMENTARIOS POSITIVOS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9AdyF3DdT5R4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Drop rows with np.nan in 'Review Text Positive' and reset index\n",
        "new_df = df.dropna(subset=['Review Text Positive']).copy()\n",
        "new_df.reset_index(inplace=True)  # reset index but keep the old index\n",
        "\n",
        "# Define your language and number of rows (you can set them to '' and None respectively if you don't want any filter)\n",
        "language = '' #Escribe '' si no quieres filtrar por país.\n",
        "num_rows = None #escribe None si no quieres filtrar por número de filas.\n",
        "\n",
        "# Filter by language if one is selected\n",
        "if language != '':\n",
        "    new_df_filtered = new_df.loc[new_df['Language_Review Text Positive'] == language].copy()\n",
        "else:\n",
        "    new_df_filtered = new_df.copy()\n",
        "\n",
        "# Select the first num_rows records if num_rows is defined\n",
        "if num_rows is not None:\n",
        "    new_df_selected = new_df_filtered[:num_rows].copy()\n",
        "else:\n",
        "    new_df_selected = new_df_filtered.copy()\n",
        "\n",
        "# Define your categories\n",
        "categories = ['location or nearness', 'cleaning', 'smell', 'good condition of facilities', 'variety of services', 'comfort', 'silence', 'temperature', 'spaciousness of the rooms', 'kindness and friendliness of the staff', 'internet', 'wi-fi', 'language', 'food']\n",
        "\n",
        "# Initialize columns for each category in the dataframe\n",
        "for category in categories:\n",
        "    new_df_selected['Positive ' + category] = 0.0\n",
        "\n",
        "# Classify each review and save the scores in the corresponding columns\n",
        "for index in tqdm(new_df_selected.index):\n",
        "    text = new_df_selected.at[index, 'Review Text Positive']\n",
        "    result = classifier(text, categories)\n",
        "    \n",
        "    for score, label in zip(result['scores'], result['labels']):\n",
        "        new_df_selected.at[index, 'Positive ' + label] = score\n",
        "\n",
        "# Now you can merge the results back to the original dataframe using the old index\n",
        "df = df.merge(new_df_selected[['index'] + ['Positive ' + category for category in categories]], how='left', left_index=True, right_on='index')\n",
        "df.set_index('index', inplace=True)  # Reset the original index\n",
        "\n",
        "# Define new fields as sums of existing fields\n",
        "df['Positive Location'] = df['Positive location or nearness']\n",
        "df['Positive Cleaning'] = df['Positive cleaning'] + df['Positive smell']\n",
        "df['Positive Condition of facilities'] = df['Positive good condition of facilities']\n",
        "df['Positive Variety of services'] = df['Positive variety of services']\n",
        "df['Positive Comfort'] = df['Positive comfort'] + df['Positive silence'] + df['Positive temperature'] + df['Positive spaciousness of the rooms']\n",
        "df['Positive Staff'] = df['Positive kindness and friendliness of the staff'] + df['Positive language']\n",
        "df['Positive Wi-fi'] = df['Positive internet'] + df['Positive wi-fi']\n",
        "df['Positive Food'] = df['Positive food']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSbrek9A9663"
      },
      "source": [
        "PARA LOS COMENTARIOS NEGATIVOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q66NHVF295im"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Drop rows with np.nan in 'Review Text Negative' and reset index\n",
        "new_df = df.dropna(subset=['Review Text Negative']).copy()\n",
        "new_df.reset_index(inplace=True)  # reset index but keep the old index\n",
        "\n",
        "# Define your language and number of rows (you can set them to '' and None respectively if you don't want any filter)\n",
        "language = '' #Escribe '' si no quieres filtrar por país.\n",
        "num_rows = None #escribe None si no quieres filtrar por número de filas.\n",
        "\n",
        "# Filter by language if one is selected\n",
        "if language != '':\n",
        "    new_df_filtered = new_df.loc[new_df['Language_Review Text Negative'] == language].copy()\n",
        "else:\n",
        "    new_df_filtered = new_df.copy()\n",
        "\n",
        "# Select the first num_rows records if num_rows is defined\n",
        "if num_rows is not None:\n",
        "    new_df_selected = new_df_filtered[:num_rows].copy()\n",
        "else:\n",
        "    new_df_selected = new_df_filtered.copy()\n",
        "\n",
        "# Define your categories\n",
        "categories = ['location or distance', 'filth', 'insects', 'dust', 'bad smell', 'uncomfortable', 'noise', 'small rooms', 'temperature', 'scarce services', 'poor condition of facilities', 'no internet', 'no wi-fi', 'unhelpful staff', 'rude staff', 'language', 'food']\n",
        "\n",
        "# Initialize columns for each category in the dataframe\n",
        "for category in categories:\n",
        "    new_df_selected['Negative ' + category] = 0.0\n",
        "\n",
        "# Classify each review and save the scores in the corresponding columns\n",
        "for index in tqdm(new_df_selected.index):\n",
        "    text = new_df_selected.at[index, 'Review Text Negative']\n",
        "    result = classifier(text, categories)\n",
        "    \n",
        "    for score, label in zip(result['scores'], result['labels']):\n",
        "        new_df_selected.at[index, 'Negative ' + label] = score\n",
        "\n",
        "# Now you can merge the results back to the original dataframe using the old index\n",
        "df = df.merge(new_df_selected[['index'] + ['Negative ' + category for category in categories]], how='left', left_index=True, right_on='index')\n",
        "df.set_index('index', inplace=True)  # Reset the original index\n",
        "\n",
        "# Creating the new variables\n",
        "df['Negative Location'] = df['Negative location or distance']\n",
        "df['Negative Food'] = df['Negative food']\n",
        "df['Negative Cleaning'] = df[['Negative filth', 'Negative insects', 'Negative dust', 'Negative bad smell']].sum(axis=1)\n",
        "df['Negative Condition of facilities'] = df['Negative poor condition of facilities']\n",
        "df['Negative Variety of services'] = df['Negative scarce services']\n",
        "df['Negative Comfort'] = df[['Negative uncomfortable', 'Negative noise', 'Negative small rooms', 'Negative temperature']].sum(axis=1)\n",
        "df['Negative Staff'] = df[['Negative unhelpful staff', 'Negative rude staff', 'Negative language']].sum(axis=1)\n",
        "df['Negative Wi-fi'] = df[['Negative no internet', 'Negative no wi-fi']].sum(axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UFLEZAuMx1f"
      },
      "outputs": [],
      "source": [
        "cols_to_fill = ['Positive Location', 'Negative Location',\n",
        "                'Positive Food', 'Negative Food',\n",
        "                'Positive Cleaning', 'Negative Cleaning',\n",
        "                'Positive Condition of facilities', 'Negative Condition of facilities',\n",
        "                'Positive Variety of services', 'Negative Variety of services',\n",
        "                'Positive Comfort', 'Negative Comfort',\n",
        "                'Positive Staff', 'Negative Staff',\n",
        "                'Positive Wi-fi', 'Negative Wi-fi']\n",
        "\n",
        "df[cols_to_fill] = df[cols_to_fill].fillna(0)\n",
        "\n",
        "df['Net Location'] = df['Positive Location'] - df['Negative Location']\n",
        "df['Net Food'] = df['Positive Food'] - df['Negative Food']\n",
        "df['Net Cleaning'] = df['Positive Cleaning'] - df['Negative Cleaning']\n",
        "df['Net Condition of facilities'] = df['Positive Condition of facilities'] - df['Negative Condition of facilities']\n",
        "df['Net Variety of services'] = df['Positive Variety of services'] - df['Negative Variety of services']\n",
        "df['Net Comfort'] = df['Positive Comfort'] - df['Negative Comfort']\n",
        "df['Net Staff'] = df['Positive Staff'] - df['Negative Staff']\n",
        "df['Net Wi-fi'] = df['Positive Wi-fi'] - df['Negative Wi-fi']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxI5VDM2kJQr"
      },
      "outputs": [],
      "source": [
        "print(df.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comprobamos la calidad del etiquetado en un subdataset filtrado por el idioma (ej. es=español, en=inglés)"
      ],
      "metadata": {
        "id": "j7tUvP_Y_xsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ajustar las opciones de visualización para mostrar todas las columnas\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', 320)  # puedes ajustar este número según tus necesidades\n"
      ],
      "metadata": {
        "id": "9vbvp_ORyqm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JL6OypIxiqV"
      },
      "outputs": [],
      "source": [
        "filtered_df = df[(df['Language_Review Text Negative'] == 'es') | (df['Language_Review Text Positive'] == 'es')]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar las primeras 10 filas en formato tabular de todos los campos\n",
        "filtered_df.head(10)"
      ],
      "metadata": {
        "id": "XB5U9u5K6g7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar las primeras 10 filas en formato tabular de todos los campos del dataframe filtrado\n",
        "filtered_df.head(10)"
      ],
      "metadata": {
        "id": "t8Kym4gGmQwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar las primeras 10 filas en formato tabular de los campos negativos\n",
        "filtered_df[['Review Text Negative', 'Negative Location', 'Negative Food', 'Negative Cleaning', 'Negative Condition of facilities', 'Negative Variety of services', 'Negative Comfort', 'Negative Staff', 'Negative Wi-fi']].head(10)\n"
      ],
      "metadata": {
        "id": "elaO-jx30n7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar las primeras 10 filas en formato tabular de los campos negativos\n",
        "filtered_df[['Review Text Positive', 'Positive Location', 'Positive Food', 'Positive Cleaning', 'Positive Condition of facilities', 'Positive Variety of services', 'Positive Comfort', 'Positive Staff', 'Positive Wi-fi']].head(10)\n"
      ],
      "metadata": {
        "id": "5u1lBiHMfC6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df[['Review Text Negative', 'Review Text Positive', 'Net Location', 'Net Food', 'Net Cleaning', 'Net Condition of facilities', 'Net Variety of services', 'Net Comfort', 'Net Staff', 'Net Wi-fi']].head(10)\n"
      ],
      "metadata": {
        "id": "Ur3nhhqsyvs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imprimir las puntuaciones netas filtradas\n",
        "print(filtered_df[['Review Text Negative', 'Review Text Positive', 'Net Location', 'Net Food', 'Net Cleaning', 'Net Condition of facilities', 'Net Variety of services', 'Net Comfort', 'Net Staff', 'Net Wi-fi']].head(10))"
      ],
      "metadata": {
        "id": "6I_NV6uc6phN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGwSEKDksO_o"
      },
      "outputs": [],
      "source": [
        "# imprimir las puntuaciones netas en el dataframe df\n",
        "print(df[['Net Location', 'Net Food', 'Net Cleaning', 'Net Condition of facilities', 'Net Variety of services', 'Net Comfort', 'Net Staff', 'Net Wi-fi']].head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bW2GK_5khIiP"
      },
      "source": [
        "UNA VEZ HAYA FUNCIONADO. GUARDAR EN UN NUEVO CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fz4sLnrUdAM0"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"/content/drive/MyDrive/comments_with_labels.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ESTADÍSTICA DESCRIPTIVA"
      ],
      "metadata": {
        "id": "ypb--quT20Mo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Contar el número de reseñas positivas y negativas"
      ],
      "metadata": {
        "id": "TVQxWsw2S3zJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtramos el dataframe para quedarnos solo con las reseñas que no son vacías\n",
        "df_positivas = df[df['Review Text Positive'] != \"\"]\n",
        "df_negativas = df[df['Review Text Negative'] != \"\"]\n",
        "\n",
        "# Contamos el número de reseñas positivas y negativas\n",
        "num_reseñas_positivas = len(df_positivas)\n",
        "num_reseñas_negativas = len(df_negativas)\n",
        "\n",
        "# Sumamos el número de reseñas positivas y negativas\n",
        "suma_reseñas = num_reseñas_positivas + num_reseñas_negativas\n",
        "\n",
        "print('Número de reseñas positivas:', num_reseñas_positivas)\n",
        "print('Número de reseñas negativas:', num_reseñas_negativas)\n",
        "print('Suma de reseñas positivas y negativas:', suma_reseñas)"
      ],
      "metadata": {
        "id": "He67rkjtS2HJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Contar el número de hoteles en la base de datos"
      ],
      "metadata": {
        "id": "jUiX4D9QU4hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtenemos el número de hoteles distintos\n",
        "num_hoteles_distintos = df['Hotel Name'].nunique()\n",
        "\n",
        "print('Número de hoteles distintos:', num_hoteles_distintos)"
      ],
      "metadata": {
        "id": "kC5ZK3S5U88t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac922d77-3e72-40bb-9a55-2d54444e3806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de hoteles distintos: 26212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Contar el número de paises en la base de datos"
      ],
      "metadata": {
        "id": "0coqCnpUa4l9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtenemos el número de paises distintos\n",
        "num_paises_distintos = df['country'].nunique()\n",
        "\n",
        "print('Número de paises distintos:', num_paises_distintos)"
      ],
      "metadata": {
        "id": "f5SvY8NlaghW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Contar el número de clientes en la base de datos"
      ],
      "metadata": {
        "id": "P-P2wMSHdBPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtenemos el número de clientes distintos\n",
        "num_clientes_distintos = df['Review Author Name'].nunique()\n",
        "\n",
        "print('Número de clientes distintos:', num_clientes_distintos)"
      ],
      "metadata": {
        "id": "UzzHdAbVc_Qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fechas que abarca el dataset"
      ],
      "metadata": {
        "id": "T0_HMjDtgU6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aseguramos que 'Review Publication Date' sea interpretada como fecha\n",
        "df['Review Publication Date'] = pd.to_datetime(df['Review Publication Date'])\n",
        "\n",
        "# Obtenemos la fecha más antigua y la más reciente\n",
        "fecha_minima = df['Review Publication Date'].min()\n",
        "fecha_maxima = df['Review Publication Date'].max()\n",
        "\n",
        "print('Fecha más antigua:', fecha_minima)\n",
        "print('Fecha más reciente:', fecha_maxima)\n"
      ],
      "metadata": {
        "id": "oJmhbdv7gUCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distribución de edades"
      ],
      "metadata": {
        "id": "hGirWq8bi7oz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Invertimos el diccionario para mapear de números a etiquetas\n",
        "inv_age_map = {v: k for k, v in age_map.items()}\n",
        "\n",
        "# Obtenemos la distribución de frecuencia relativa de las edades, la convertimos a porcentajes y la redondeamos\n",
        "distribucion_edades = (df['Review Author Age'].value_counts(normalize=True).sort_index() * 100).round(1)\n",
        "\n",
        "# Mapeamos los números a las etiquetas\n",
        "distribucion_edades.index = distribucion_edades.index.map(inv_age_map)\n",
        "\n",
        "# Convertimos los valores a cadenas y agregamos el símbolo de porcentaje\n",
        "distribucion_edades = distribucion_edades.apply(lambda x: f'{x}%')\n",
        "\n",
        "print(distribucion_edades)\n",
        "\n"
      ],
      "metadata": {
        "id": "eDrJO2AKhMPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resultados de rating"
      ],
      "metadata": {
        "id": "Uamg9l00rWRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculamos la media y la redondeamos a un decimal\n",
        "media = round(df['Review Rating'].mean(), 1)\n",
        "\n",
        "# Calculamos la mediana y la redondeamos a un decimal\n",
        "mediana = round(df['Review Rating'].median(), 1)\n",
        "\n",
        "# Calculamos el rango intercuartílico (IQR) y lo redondeamos a un decimal\n",
        "Q1 = df['Review Rating'].quantile(0.25)\n",
        "Q3 = df['Review Rating'].quantile(0.75)\n",
        "IQR = round(Q3 - Q1, 1)\n",
        "\n",
        "print('Media:', media)\n",
        "print('Mediana:', mediana)\n",
        "print('p25:', Q1)\n",
        "print('p75:', Q3)\n",
        "print('Rango intercuartílico:', IQR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwkXDiDskbmn",
        "outputId": "28ed4923-e7ba-4268-9f63-6907cfd68723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Media: 8.0\n",
            "Mediana: 8.3\n",
            "p25: 6.7\n",
            "p75: 9.6\n",
            "Rango intercuartílico: 2.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rating en función de la edad"
      ],
      "metadata": {
        "id": "ofTCO9v0lu-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapeamos los números a las etiquetas de las edades\n",
        "df['Review Author Age'] = df['Review Author Age'].map(inv_age_map)\n",
        "\n",
        "# Agrupamos los datos por 'Review Author Age' y calculamos la media, mediana y rango intercuartílico (IQR)\n",
        "media = df.groupby('Review Author Age')['Review Rating'].mean().round(1)\n",
        "mediana = df.groupby('Review Author Age')['Review Rating'].median().round(1)\n",
        "IQR = (df.groupby('Review Author Age')['Review Rating'].quantile(0.75) - df.groupby('Review Author Age')['Review Rating'].quantile(0.25)).round(1)\n",
        "\n",
        "print('Media:\\n', media)\n",
        "print('\\nMediana:\\n', mediana)\n",
        "print('p25:', Q1)\n",
        "print('p75:', Q3)\n",
        "print('\\nRango intercuartílico:\\n', IQR)\n"
      ],
      "metadata": {
        "id": "BugarBz6lt-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mostramos los paises con más y menos rating."
      ],
      "metadata": {
        "id": "Ohxb7rUbm6sE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Agrupamos los datos por 'country' y calculamos el rating promedio, excluyendo los valores NaN\n",
        "average_rating_by_country = df.groupby('country')['Review Rating'].mean().dropna()\n",
        "\n",
        "# Ordenamos los resultados\n",
        "average_rating_by_country = average_rating_by_country.sort_values()\n",
        "\n",
        "# Seleccionamos los 5 países con los ratings más bajos\n",
        "lowest_ratings = average_rating_by_country.head(5).round(1)\n",
        "\n",
        "# Seleccionamos los 5 países con los ratings más altos\n",
        "highest_ratings = average_rating_by_country.tail(5).round(1)\n",
        "\n",
        "print('5 países con los ratings más bajos:\\n', lowest_ratings)\n",
        "print('\\n5 países con los ratings más altos:\\n', highest_ratings)\n",
        "\n"
      ],
      "metadata": {
        "id": "HiF1PUvVm5fG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Idioma empleado en las review positivas"
      ],
      "metadata": {
        "id": "zRzuTGvErcEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculamos las frecuencias de las categorías\n",
        "category_frequencies = df['Language_Review Text Positive'].value_counts()\n",
        "\n",
        "# Calculamos el porcentaje de frecuencia\n",
        "category_percentages = (category_frequencies / df['Language_Review Text Positive'].count() * 100).round(1)\n",
        "\n",
        "# Seleccionamos las 5 categorías más frecuentes\n",
        "top_5_categories = category_percentages.head(5)\n",
        "\n",
        "print('Las 5 categorías más frecuentes en \"Language_Review Text Positive\" son:\\n', top_5_categories)\n"
      ],
      "metadata": {
        "id": "l8VIsia8raVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCfs6K_UPfM-"
      },
      "source": [
        "# EXPLOTACIÓN Y VISUALIZACIÓN DE DATOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tn7y5DXSwedy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/comments_with_labels.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Para ver por qué paises podemos seleccionar podemos ejecutar el siguiente código:\n",
        "pd.set_option('display.max_rows', None)\n",
        "df['country'].value_counts()\n"
      ],
      "metadata": {
        "id": "zw8NTL1tJ2iW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFWltpPkIWp5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Solicitar al usuario el país seleccionado\n",
        "selected_country = input(\"Ingrese el país seleccionado: \")\n",
        "\n",
        "# Agrupamos por país y calculamos el tamaño de la burbuja (número de registros por país) y las medias de las columnas\n",
        "grouped = df.groupby('country').agg({'Review Rating': 'mean','Net Location': 'mean','Net Food': 'mean','Net Cleaning': 'mean','Net Condition of facilities': 'mean','Net Variety of services': 'mean','Net Comfort': 'mean','Net Staff': 'mean','Net Wi-fi': 'mean','country': 'count'})\n",
        "\n",
        "grouped.columns = ['Review_Mean','Net_Location_Mean','Net_Food_Mean','Net_Cleaning_Mean','Net_Condition_Mean','Net_Variety_Mean','Net_Comfort_Mean','Net_Staff_Mean','Net_Wifi_Mean','Count']\n",
        "\n",
        "# Crear una lista de las columnas a mostrar en subgráficos\n",
        "columns = ['Net_Location_Mean','Net_Food_Mean','Net_Cleaning_Mean','Net_Condition_Mean','Net_Variety_Mean','Net_Comfort_Mean','Net_Staff_Mean','Net_Wifi_Mean']\n",
        "\n",
        "# Configurar la disposición de los subgráficos\n",
        "num_columns = 4\n",
        "num_rows = (len(columns) + num_columns - 1) // num_columns\n",
        "fig, axs = plt.subplots(num_rows, num_columns, figsize=(12, 8))\n",
        "\n",
        "# Establecer límites en el eje x\n",
        "for ax in axs.flat:\n",
        "    ax.set_xlim(-0.5, 0.5)\n",
        "\n",
        "# Iterar sobre las columnas y crear subgráficos\n",
        "for i, column in enumerate(columns):\n",
        "    row = i // num_columns\n",
        "    col = i % num_columns\n",
        "    ax = axs[row, col]\n",
        "\n",
        "    # Dibujar el gráfico de burbujas para la columna actual\n",
        "    ax.scatter(grouped[column], grouped['Review_Mean'], s=grouped['Count'], alpha=0.5, color='blue')\n",
        "\n",
        "    # Filtrar el DataFrame para el país seleccionado\n",
        "    filtered_df = grouped[grouped.index == selected_country]\n",
        "\n",
        "    # Dibujar el país seleccionado en color rojo\n",
        "    ax.scatter(filtered_df[column], filtered_df['Review_Mean'], s=filtered_df['Count'], alpha=0.7, color='red')\n",
        "\n",
        "    # Etiquetar los ejes y el título del subgráfico\n",
        "    ax.set_xlabel(column)\n",
        "    ax.set_ylabel('Review Rating Mean')\n",
        "    ax.set_title(column)\n",
        "\n",
        "    # Dibujar una línea vertical en x=0\n",
        "    ax.axvline(0, linestyle='--', color='gray')  # Añade esta línea\n",
        "\n",
        "\n",
        "# Ajustar los espacios entre los subgráficos y ocultar los subgráficos no utilizados\n",
        "fig.tight_layout()\n",
        "for i in range(len(columns), num_rows * num_columns):\n",
        "    axs.flatten()[i].axis('off')\n",
        "\n",
        "# Mostrar el gráfico\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}